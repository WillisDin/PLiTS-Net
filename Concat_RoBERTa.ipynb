{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup & Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Cell 2: Configuration (Optimized for Concat Model)\n",
    "class Config:\n",
    "    # Model: RoBERTa-wwm-ext\n",
    "    MODEL_NAME = \"hfl/chinese-roberta-wwm-ext\"\n",
    "    DATA_PATH = \"DATA_PATH\"  # Modify to the actual data file name and path after parsing.\n",
    "    \n",
    "    # --- Optimized Hyperparameters ---\n",
    "    # 1. Batch Size: Kept small (16) to handle longer sequence lengths in Concat mode\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    # 2. Learning Rate:  3e-4\n",
    "    LEARNING_RATE = 3e-4\n",
    "    \n",
    "    # 3. Epochs: 10 (Allow convergence with lower LR)\n",
    "    EPOCHS = 10\n",
    "    \n",
    "    # 4. Max Length: Increased to 512.\n",
    "    # The paper notes PLMs are limited to 512 tokens.\n",
    "    # Concat model needs max length to fit context + target.\n",
    "    MAX_LEN = 256 \n",
    "    \n",
    "    GRADIENT_CLIPPING = 1.0\n",
    "    LABEL_MAP = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Cell 3: Data Preprocessing (Context Construction)\n",
    "# This is the key difference from Flat RoBERTa.\n",
    "# We must reconstruct the thread history for each post.\n",
    "\n",
    "def load_and_construct_context(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df = df.dropna(subset=['text', 'sentiment', 'thread_id', 'turn_index'])\n",
    "    df['label'] = df['sentiment'].map(config.LABEL_MAP)\n",
    "    df = df.dropna(subset=['label'])\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    \n",
    "    # Sort to ensure order: thread_id -> turn_index\n",
    "    df = df.sort_values(by=['thread_id', 'turn_index'])\n",
    "    \n",
    "    context_texts = []\n",
    "    target_texts = []\n",
    "    \n",
    "    # Group by thread to process conversation history\n",
    "    # The paper mentions concatenating target post with \"parent thread\" \n",
    "    grouped = df.groupby('thread_id')\n",
    "    \n",
    "    for _, group in tqdm(grouped, desc=\"Constructing Threads\"):\n",
    "        # Convert group to list of texts\n",
    "        texts = group['text'].tolist()\n",
    "        \n",
    "        # Determine context for each post in the thread\n",
    "        # For turn 0: Context is empty (or special token)\n",
    "        # For turn N: Context is concatenation of 0 to N-1\n",
    "        current_thread_history = \"\"\n",
    "        \n",
    "        for i in range(len(texts)):\n",
    "            target = str(texts[i])\n",
    "            \n",
    "            # Use [SEP] as a separator for history in raw text if needed, \n",
    "            # though tokenizer handles structure better.\n",
    "            # Here we just accumulate plain text history.\n",
    "            context = current_thread_history\n",
    "            \n",
    "            context_texts.append(context)\n",
    "            target_texts.append(target)\n",
    "            \n",
    "            # Update history for the NEXT turn\n",
    "            # We limit history length roughly here to avoid massive string ops, \n",
    "            # though Tokenizer will do the hard truncation.\n",
    "            current_thread_history += target + \" [SEP] \" \n",
    "\n",
    "    # Re-assign back to dataframe to keep alignment\n",
    "    # Note: simple append works because we iterate groups in order and df was sorted\n",
    "    df['context_text'] = context_texts\n",
    "    df['target_text'] = target_texts\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Processing data to construct conversation context...\")\n",
    "df = load_and_construct_context(config.DATA_PATH)\n",
    "print(f\"Total processed samples: {len(df)}\")\n",
    "print(\"Sample Context (Row 1):\", df.iloc[1]['context_text'])\n",
    "print(\"Sample Target  (Row 1):\", df.iloc[1]['target_text'])\n",
    "\n",
    "# Split by thread_id\n",
    "unique_threads = df['thread_id'].unique()\n",
    "train_threads, test_threads = train_test_split(unique_threads, test_size=0.2, random_state=42)\n",
    "train_threads, val_threads = train_test_split(train_threads, test_size=0.1, random_state=42)\n",
    "\n",
    "train_df = df[df['thread_id'].isin(train_threads)].reset_index(drop=True)\n",
    "val_df = df[df['thread_id'].isin(val_threads)].reset_index(drop=True)\n",
    "test_df = df[df['thread_id'].isin(test_threads)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Cell 4: Compute Class Weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label']\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# Cell 5: Concat Dataset Class\n",
    "class ConcatWeiboDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.context = dataframe['context_text']\n",
    "        self.target = dataframe['target_text']\n",
    "        self.labels = dataframe['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        context_text = str(self.context[index])\n",
    "        target_text = str(self.target[index])\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # Use tokenizer to handle Sentence Pair classification\n",
    "        # text_a = context, text_b = target\n",
    "        # BERT Input: [CLS] context [SEP] target [SEP]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text=context_text,\n",
    "            text_pair=target_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True, # Important for Concat to distinguish segments\n",
    "            truncation=True, # Truncates context if too long, usually\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'targets': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "train_dataset = ConcatWeiboDataset(train_df, tokenizer, config.MAX_LEN)\n",
    "val_dataset = ConcatWeiboDataset(val_df, tokenizer, config.MAX_LEN)\n",
    "test_dataset = ConcatWeiboDataset(test_df, tokenizer, config.MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Cell 6: Concat RoBERTa Model Architecture\n",
    "class ConcatRoBERTa(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(ConcatRoBERTa, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.MODEL_NAME)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # We also pass token_type_ids here so RoBERTa knows which is context and which is target\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.classifier(output)\n",
    "\n",
    "model = ConcatRoBERTa(config.NUM_CLASSES).to(device)\n",
    "\n",
    "# Cell 7: Training Setup\n",
    "optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights) # Weighted Loss\n",
    "\n",
    "total_steps = len(train_loader) * config.EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(total_steps * 0.1), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Cell 8: Training Loop\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.GRADIENT_CLIPPING)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses), f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "print(\"Starting Concat RoBERTa Training (Optimized)...\")\n",
    "best_macro_f1 = 0\n",
    "for epoch in range(config.EPOCHS):\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device, scheduler)\n",
    "    val_acc, val_loss, val_f1 = eval_model(model, val_loader, loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}/{config.EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    if val_f1 > best_macro_f1:\n",
    "        best_macro_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'optimized_concat_roberta.bin')\n",
    "        print(\"=> Saved Best Model\")\n",
    "\n",
    "# Cell 9: Final Evaluation\n",
    "model.load_state_dict(torch.load('optimized_concat_roberta.bin'))\n",
    "test_acc, test_loss, test_f1 = eval_model(model, test_loader, loss_fn, device)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Final Test Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "# Detailed Report\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for d in test_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[\"Negative\", \"Neutral\", \"Positive\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d9985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070e6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3b2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d114f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73dc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21d2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8be8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e06f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6728b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c6690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d99a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47054c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bigcomp)",
   "language": "python",
   "name": "bigcomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
