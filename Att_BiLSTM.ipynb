{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90302f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup & Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Cell 2: Configuration\n",
    "class Config:\n",
    "    # Model backbone\n",
    "    MODEL_NAME = \"hfl/chinese-roberta-wwm-ext\"\n",
    "    DATA_PATH = \"DATA_PATH\"  # Modify to the actual data file name and path after parsing.\n",
    "    \n",
    "    # --- FIXED Hyperparameters ---\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 0.0003 \n",
    "    EPOCHS = 10\n",
    "    DROPOUT = 0.3\n",
    "    MAX_POST_LEN = 256\n",
    "    GRADIENT_CLIPPING = 1.0\n",
    "    \n",
    "    # --- Architecture Parameters ---\n",
    "    EMBEDDING_DIM = 768\n",
    "    HIDDEN_DIM = 768  # Match BERT output\n",
    "    LSTM_LAYERS = 2\n",
    "    NUM_ATTENTION_HEADS = 8  # Multi-head attention\n",
    "    \n",
    "    # --- Fine-tuning Config ---\n",
    "    UNFREEZE_LAYERS = 3  # Unfreeze last 3 BERT layers\n",
    "    BERT_LR = 0.0003  # Same as base LR for simplicity\n",
    "    \n",
    "    # --- Advanced Training Config ---\n",
    "    WARMUP_RATIO = 0.15\n",
    "    USE_FOCAL_LOSS = False\n",
    "    FOCAL_ALPHA = 0.25\n",
    "    FOCAL_GAMMA = 2.0\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "    \n",
    "    LABEL_MAP = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Cell 3: Data Loading & Robust Thread Structuring\n",
    "def load_thread_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Critical: Drop rows where ANY essential info is missing\n",
    "    df = df.dropna(subset=['text', 'sentiment', 'thread_id', 'turn_index'])\n",
    "    \n",
    "    # Ensure types\n",
    "    df['label'] = df['sentiment'].map(config.LABEL_MAP)\n",
    "    df = df.dropna(subset=['label'])\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    \n",
    "    # Sort strictly\n",
    "    df = df.sort_values(by=['thread_id', 'turn_index'])\n",
    "    \n",
    "    threads = []\n",
    "    labels = []\n",
    "    \n",
    "    # Group extraction\n",
    "    grouped = df.groupby('thread_id')\n",
    "    for thread_id, group in grouped:\n",
    "        group_texts = group['text'].tolist()\n",
    "        group_labels = group['label'].tolist()\n",
    "        \n",
    "        if len(group_texts) != len(group_labels):\n",
    "            print(f\"Warning: Thread {thread_id} mismatch. Texts: {len(group_texts)}, Labels: {len(group_labels)}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        threads.append(group_texts)\n",
    "        labels.append(group_labels)\n",
    "        \n",
    "    return threads, labels, df['label'].values\n",
    "\n",
    "print(\"Loading data with integrity checks...\")\n",
    "threads, thread_labels, all_labels = load_thread_data(config.DATA_PATH)\n",
    "print(f\"Successfully loaded {len(threads)} threads.\")\n",
    "\n",
    "# Split Dataset\n",
    "train_texts, test_texts, train_y, test_y = train_test_split(\n",
    "    threads, thread_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "train_texts, val_texts, train_y, val_y = train_test_split(\n",
    "    train_texts, train_y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train Threads: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")\n",
    "\n",
    "# Class Weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(all_labels),\n",
    "    y=all_labels\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# Cell 4: Dataset and Collate Function\n",
    "class ThreadDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        thread_posts = self.texts[index]\n",
    "        thread_labels = self.labels[index]\n",
    "        \n",
    "        assert len(thread_posts) == len(thread_labels), f\"Sample {index} mismatch\"\n",
    "        \n",
    "        encoded_posts = []\n",
    "        for post in thread_posts:\n",
    "            encoded = self.tokenizer.encode_plus(\n",
    "                str(post),\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            encoded_posts.append({\n",
    "                'input_ids': encoded['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoded['attention_mask'].squeeze(0)\n",
    "            })\n",
    "            \n",
    "        return {\n",
    "            'encoded_posts': encoded_posts,\n",
    "            'labels': thread_labels,\n",
    "            'thread_len': len(thread_posts)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_thread_len = max(item['thread_len'] for item in batch)\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    padded_input_ids = torch.zeros(batch_size, max_thread_len, config.MAX_POST_LEN, dtype=torch.long)\n",
    "    padded_attention_masks = torch.zeros(batch_size, max_thread_len, config.MAX_POST_LEN, dtype=torch.long)\n",
    "    padded_labels = torch.full((batch_size, max_thread_len), -100, dtype=torch.long)\n",
    "    lengths = []\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        posts = item['encoded_posts']\n",
    "        labels = item['labels']\n",
    "        thread_len = item['thread_len']\n",
    "        \n",
    "        assert len(posts) == len(labels) == thread_len, \\\n",
    "            f\"Batch item {i}: posts {len(posts)}, labels {len(labels)}, thread_len {thread_len}\"\n",
    "        \n",
    "        for j in range(thread_len):\n",
    "            padded_input_ids[i, j] = posts[j]['input_ids']\n",
    "            padded_attention_masks[i, j] = posts[j]['attention_mask']\n",
    "            padded_labels[i, j] = labels[j]\n",
    "        \n",
    "        lengths.append(thread_len)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': padded_input_ids,\n",
    "        'attention_mask': padded_attention_masks,\n",
    "        'labels': padded_labels,\n",
    "        'lengths': torch.tensor(lengths, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "train_dataset = ThreadDataset(train_texts, train_y, tokenizer, config.MAX_POST_LEN)\n",
    "val_dataset = ThreadDataset(val_texts, val_y, tokenizer, config.MAX_POST_LEN)\n",
    "test_dataset = ThreadDataset(test_texts, test_y, tokenizer, config.MAX_POST_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"\\nData loaders created successfully!\")\n",
    "\n",
    "# Cell 5: Focal Loss Implementation\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, weight=None, ignore_index=-100):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, \n",
    "                                   ignore_index=self.ignore_index, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Cell 6: Multi-Head Self-Attention Module\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.scale = 1.0 / np.sqrt(self.head_dim)\n",
    "        \n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, seq_len]\n",
    "            scores = scores.masked_fill(~mask, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Output projection and residual\n",
    "        output = self.out_proj(context)\n",
    "        output = self.layer_norm(x + self.dropout(output))\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Cell 7: Enhanced Attn-BiLSTM Model with Fine-tuning\n",
    "class EnhancedAttnBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EnhancedAttnBiLSTM, self).__init__()\n",
    "        \n",
    "        # Load BERT and selectively unfreeze\n",
    "        self.bert = BertModel.from_pretrained(config.MODEL_NAME)\n",
    "        self._freeze_bert_selectively(config.UNFREEZE_LAYERS)\n",
    "        \n",
    "        # Feature fusion layer\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(config.EMBEDDING_DIM, config.EMBEDDING_DIM),\n",
    "            nn.LayerNorm(config.EMBEDDING_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.DROPOUT)\n",
    "        )\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=config.EMBEDDING_DIM,\n",
    "            hidden_size=config.HIDDEN_DIM // 2,\n",
    "            num_layers=config.LSTM_LAYERS,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=config.DROPOUT if config.LSTM_LAYERS > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Multi-Head Self-Attention\n",
    "        self.attention = MultiHeadSelfAttention(\n",
    "            config.HIDDEN_DIM, \n",
    "            num_heads=config.NUM_ATTENTION_HEADS, \n",
    "            dropout=config.DROPOUT\n",
    "        )\n",
    "        \n",
    "        # Enhanced Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(config.HIDDEN_DIM, config.HIDDEN_DIM // 2),\n",
    "            nn.LayerNorm(config.HIDDEN_DIM // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.HIDDEN_DIM // 2, config.NUM_CLASSES)\n",
    "        )\n",
    "        \n",
    "    def _freeze_bert_selectively(self, unfreeze_layers):\n",
    "        # Freeze all layers first\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze last N encoder layers\n",
    "        if unfreeze_layers > 0:\n",
    "            for layer in self.bert.encoder.layer[-unfreeze_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "            \n",
    "            # Unfreeze pooler\n",
    "            for param in self.bert.pooler.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "            print(f\"✓ Unfroze last {unfreeze_layers} BERT layers for fine-tuning\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, lengths):\n",
    "        batch_size, max_thread_len, max_post_len = input_ids.shape\n",
    "        \n",
    "        # Flatten for BERT\n",
    "        flat_input_ids = input_ids.view(batch_size * max_thread_len, max_post_len)\n",
    "        flat_masks = attention_mask.view(batch_size * max_thread_len, max_post_len)\n",
    "        \n",
    "        # BERT encoding (with gradient for unfrozen layers)\n",
    "        outputs = self.bert(flat_input_ids, flat_masks)\n",
    "        post_embeddings = outputs.pooler_output\n",
    "        \n",
    "        # Feature fusion\n",
    "        post_embeddings = self.feature_fusion(post_embeddings)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        lstm_input = post_embeddings.view(batch_size, max_thread_len, -1)\n",
    "        \n",
    "        # BiLSTM\n",
    "        packed_input = pack_padded_sequence(\n",
    "            lstm_input, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        lstm_output, _ = pad_packed_sequence(\n",
    "            packed_output, batch_first=True, total_length=max_thread_len\n",
    "        )\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        thread_mask = torch.arange(max_thread_len, device=input_ids.device)[None, :] < lengths[:, None].to(input_ids.device)\n",
    "        attn_output = self.attention(lstm_output, mask=thread_mask)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(attn_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Cell 8: Initialize Model\n",
    "model = EnhancedAttnBiLSTM(config).to(device)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable ratio: {trainable_params/total_params*100:.2f}%\")\n",
    "\n",
    "# Cell 9: Setup Training Components\n",
    "# Separate parameter groups for different learning rates\n",
    "bert_params = []\n",
    "other_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if 'bert' in name:\n",
    "            bert_params.append(param)\n",
    "        else:\n",
    "            other_params.append(param)\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {'params': bert_params, 'lr': config.BERT_LR},\n",
    "    {'params': other_params, 'lr': config.LEARNING_RATE}\n",
    "])\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * config.EPOCHS\n",
    "warmup_steps = int(total_steps * config.WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "if config.USE_FOCAL_LOSS:\n",
    "    loss_fn = FocalLoss(\n",
    "        alpha=config.FOCAL_ALPHA,\n",
    "        gamma=config.FOCAL_GAMMA,\n",
    "        weight=class_weights,\n",
    "        ignore_index=-100\n",
    "    )\n",
    "    print(f\"✓ Using Focal Loss (alpha={config.FOCAL_ALPHA}, gamma={config.FOCAL_GAMMA})\")\n",
    "else:\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "    print(\"✓ Using CrossEntropy Loss\")\n",
    "\n",
    "print(f\"✓ Warmup steps: {warmup_steps}/{total_steps}\")\n",
    "\n",
    "# Cell 10: Training Loop\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, scheduler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\")\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        masks = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        lengths = batch['lengths']\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, masks, lengths)\n",
    "        \n",
    "        # Flatten for loss\n",
    "        active_logits = logits.view(-1, config.NUM_CLASSES)\n",
    "        active_labels = labels.view(-1)\n",
    "        \n",
    "        loss = loss_fn(active_logits, active_labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIPPING)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Metrics\n",
    "        preds = torch.argmax(active_logits, dim=1)\n",
    "        mask = active_labels != -100\n",
    "        all_preds.extend(preds[mask].cpu().numpy())\n",
    "        all_targets.extend(active_labels[mask].cpu().numpy())\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'lr': f'{current_lr:.2e}'\n",
    "            })\n",
    "        \n",
    "    return np.mean(losses), f1_score(all_targets, all_preds, average='macro'), accuracy_score(all_targets, all_preds)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            masks = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            lengths = batch['lengths']\n",
    "            \n",
    "            logits = model(input_ids, masks, lengths)\n",
    "            \n",
    "            active_logits = logits.view(-1, config.NUM_CLASSES)\n",
    "            active_labels = labels.view(-1)\n",
    "            \n",
    "            loss = loss_fn(active_logits, active_labels)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            preds = torch.argmax(active_logits, dim=1)\n",
    "            mask = active_labels != -100\n",
    "            all_preds.extend(preds[mask].cpu().numpy())\n",
    "            all_targets.extend(active_labels[mask].cpu().numpy())\n",
    "            \n",
    "    return np.mean(losses), f1_score(all_targets, all_preds, average='macro'), accuracy_score(all_targets, all_preds)\n",
    "\n",
    "# Cell 11: Training\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Starting Enhanced Attn-BiLSTM Training with Fine-tuning...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_f1 = 0\n",
    "best_acc = 0\n",
    "patience = 0\n",
    "max_patience = 5\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.EPOCHS}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    train_loss, train_f1, train_acc = train_epoch(model, train_loader, loss_fn, optimizer, scheduler)\n",
    "    val_loss, val_f1, val_acc = eval_model(model, val_loader, loss_fn)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | F1: {train_f1:.4f} | Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | F1: {val_f1:.4f} | Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_acc = val_acc\n",
    "        patience = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_f1': val_f1,\n",
    "            'val_acc': val_acc\n",
    "        }, 'best_enhanced_model.bin')\n",
    "        print(f\"✓ Saved Best Model (F1: {best_f1:.4f}, Acc: {best_acc:.4f})\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= max_patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Cell 12: Final Evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Final Evaluation on Test Set\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checkpoint = torch.load('best_enhanced_model.bin')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "test_loss, test_f1, test_acc = eval_model(model, test_loader, loss_fn)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"  Macro F1:  {test_f1:.4f}\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Detailed Report\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        masks = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        lengths = batch['lengths']\n",
    "        \n",
    "        logits = model(input_ids, masks, lengths)\n",
    "        active_logits = logits.view(-1, config.NUM_CLASSES)\n",
    "        active_labels = labels.view(-1)\n",
    "        \n",
    "        preds = torch.argmax(active_logits, dim=1)\n",
    "        mask = active_labels != -100\n",
    "        all_preds.extend(preds[mask].cpu().numpy())\n",
    "        all_targets.extend(active_labels[mask].cpu().numpy())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(all_targets, all_preds, target_names=[\"Negative\", \"Neutral\", \"Positive\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c7979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233e86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe9c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c89d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c541425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0a8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03573d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d03ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f97da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb9525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bigcomp)",
   "language": "python",
   "name": "bigcomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
