{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c014f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup & Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Cell 2: Configuration (Optimized)\n",
    "class Config:\n",
    "    # Model backbone: RoBERTa-wwm-ext for Chinese\n",
    "    MODEL_NAME = \"hfl/chinese-roberta-wwm-ext\"\n",
    "    \n",
    "    DATA_PATH = \"DATA_PATH\" # Modify to the actual data file name and path after parsing. \n",
    "    \n",
    "    # --- Optimization Changes ---\n",
    "    # 1. Smaller Batch Size for better generalization stability\n",
    "    BATCH_SIZE = 32 \n",
    "    \n",
    "    # 2. Lower Learning Rate: Standard fine-tuning range \n",
    "    # The paper's 3e-4 is often too aggressive for full fine-tuning\n",
    "    LEARNING_RATE = 3e-4 \n",
    "    \n",
    "    # 3. More Epochs with Early Stopping logic implies we give it time to converge\n",
    "    EPOCHS = 10 \n",
    "    \n",
    "    MAX_LEN = 256\n",
    "    GRADIENT_CLIPPING = 1.0\n",
    "    \n",
    "    LABEL_MAP = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Cell 3: Data Loading & Split\n",
    "df = pd.read_csv(config.DATA_PATH)\n",
    "df = df.dropna(subset=['text', 'sentiment'])\n",
    "df['label'] = df['sentiment'].map(config.LABEL_MAP)\n",
    "df = df.dropna(subset=['label'])\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Split by thread_id to prevent context leakage\n",
    "unique_threads = df['thread_id'].unique()\n",
    "train_threads, test_threads = train_test_split(unique_threads, test_size=0.2, random_state=42)\n",
    "train_threads, val_threads = train_test_split(train_threads, test_size=0.1, random_state=42)\n",
    "\n",
    "train_df = df[df['thread_id'].isin(train_threads)].reset_index(drop=True)\n",
    "val_df = df[df['thread_id'].isin(val_threads)].reset_index(drop=True)\n",
    "test_df = df[df['thread_id'].isin(test_threads)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# Cell 4: Compute Class Weights (Crucial Optimization)\n",
    "# Calculate weights to penalize the model more for missing minority classes\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label']\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# Cell 5: Dataset Class\n",
    "class WeiboSentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.text = dataframe['text']\n",
    "        self.targets = dataframe['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        target = self.targets[index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(config.MODEL_NAME)\n",
    "train_loader = DataLoader(WeiboSentimentDataset(train_df, tokenizer, config.MAX_LEN), \n",
    "                          batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(WeiboSentimentDataset(val_df, tokenizer, config.MAX_LEN), \n",
    "                        batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(WeiboSentimentDataset(test_df, tokenizer, config.MAX_LEN), \n",
    "                         batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Cell 6: Model Architecture\n",
    "class FlatRoBERTa(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(FlatRoBERTa, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.MODEL_NAME)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.classifier(output)\n",
    "\n",
    "model = FlatRoBERTa(config.NUM_CLASSES).to(device)\n",
    "\n",
    "# Cell 7: Training Setup (Optimized)\n",
    "# Use the lower learning rate defined in Config\n",
    "optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "# Use Weighted Loss to handle imbalance\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "total_steps = len(train_loader) * config.EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(total_steps * 0.1), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Cell 8: Training Loop\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.GRADIENT_CLIPPING)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses), f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "print(\"Starting Optimized Training...\")\n",
    "best_macro_f1 = 0\n",
    "for epoch in range(config.EPOCHS):\n",
    "    train_acc, train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device, scheduler)\n",
    "    val_acc, val_loss, val_f1 = eval_model(model, val_loader, loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}/{config.EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    if val_f1 > best_macro_f1:\n",
    "        best_macro_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'optimized_flat_roberta.bin')\n",
    "        print(\"=> Saved Best Model\")\n",
    "\n",
    "# Cell 9: Final Evaluation\n",
    "model.load_state_dict(torch.load('optimized_flat_roberta.bin'))\n",
    "test_acc, test_loss, test_f1 = eval_model(model, test_loader, loss_fn, device)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Final Test Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "# Detailed Report\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for d in test_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=[\"Negative\", \"Neutral\", \"Positive\"], zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf99362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14964456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca7744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d494745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed411e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437ce5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b47a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7eeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443b328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2138cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c60695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d21b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eab8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c50d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca831cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9022a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32dc252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bigcomp)",
   "language": "python",
   "name": "bigcomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
