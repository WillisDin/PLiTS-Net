{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69515c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "\n",
    "SEED = 42\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "CSV_FILE_PATH = 'DATA_PATH' # Modify to the actual data file name and path after parsing.\n",
    "PLM_MODEL_PATH = 'hfl/chinese-roberta-wwm-ext' \n",
    "MAX_LEN = 256\n",
    "print(f\"Running on: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, mid, text, label=None):\n",
    "        self.mid = mid\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        self.children = []\n",
    "        self.input_ids = None\n",
    "        self.attention_mask = None\n",
    "        # Using for storing hidden states during model processing\n",
    "        self.state_h = None\n",
    "        self.state_c = None\n",
    "\n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)\n",
    "\n",
    "def load_and_process_data(csv_path, tokenizer):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Label Mapping\n",
    "    unique_labels = df['sentiment'].unique()\n",
    "    label2id = {l: i for i, l in enumerate(unique_labels)}\n",
    "    id2label = {i: l for i, l in enumerate(unique_labels)}\n",
    "    print(f\"Label Mapping: {label2id}\")\n",
    "    \n",
    "    # Building Trees\n",
    "    trees = []\n",
    "    # Group by thread_id to build separate trees\n",
    "    grouped = df.groupby('thread_id')\n",
    "    \n",
    "    for _, group in tqdm(grouped, desc=\"Building Trees\"):\n",
    "        nodes = {}\n",
    "        # 1. Bilding nodes\n",
    "        for _, row in group.iterrows():\n",
    "            mid = row['mid']\n",
    "            text = str(row['text'])\n",
    "            label = label2id[row['sentiment']]\n",
    "            nodes[mid] = TreeNode(mid, text, label)\n",
    "            \n",
    "            # prepare tokenized inputs\n",
    "            encoding = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=MAX_LEN,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            nodes[mid].input_ids = encoding['input_ids'].squeeze(0) # (seq_len)\n",
    "            nodes[mid].attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "            \n",
    "        # 2. Building tree structure\n",
    "        roots = []\n",
    "        for _, row in group.iterrows():\n",
    "            mid = row['mid']\n",
    "            parent_id = row['parent_id']\n",
    "            current_node = nodes[mid]\n",
    "            \n",
    "            # If no parent, it's a root node\n",
    "            if parent_id == 0 or parent_id not in nodes:\n",
    "                roots.append(current_node)\n",
    "            else:\n",
    "                nodes[parent_id].add_child(current_node)\n",
    "        \n",
    "        trees.extend(roots)\n",
    "        \n",
    "    return trees, label2id, id2label\n",
    "\n",
    "# Loading tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PLM_MODEL_PATH)\n",
    "\n",
    "# Loading and processing data\n",
    "if os.path.exists(CSV_FILE_PATH):\n",
    "    all_trees, label2id, id2label = load_and_process_data(CSV_FILE_PATH, tokenizer)\n",
    "    NUM_CLASSES = len(label2id)\n",
    "    \n",
    "    # 70% Train, 15% Val, 15% Test\n",
    "    train_val, test_trees = train_test_split(all_trees, test_size=0.15, random_state=SEED, shuffle=True)\n",
    "    train_trees, val_trees = train_test_split(train_val, test_size=0.1765, random_state=SEED) \n",
    "    \n",
    "    print(f\"Total Trees: {len(all_trees)}\")\n",
    "    print(f\"Train: {len(train_trees)}, Val: {len(val_trees)}, Test: {len(test_trees)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Please upload {CSV_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChildSumTreeLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ChildSumTreeLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Gates: i, o, u, \n",
    "        # Child-Sum: h_tilde = sum(h_children)\n",
    "        self.W_iou = nn.Linear(input_dim, 3 * hidden_dim)\n",
    "        self.U_iou = nn.Linear(hidden_dim, 3 * hidden_dim, bias=False)\n",
    "        \n",
    "        # Forget gate for each child\n",
    "        self.W_f = nn.Linear(input_dim, hidden_dim)\n",
    "        self.U_f = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x, child_c, child_h):\n",
    "        \"\"\"\n",
    "        x: (1, input_dim) - current node input\n",
    "        child_c: list of (1, hidden_dim) - list of child cell states\n",
    "        child_h: list of (1, hidden_dim) - list of child hidden states\n",
    "        Returns:\n",
    "            h_new: (1, hidden_dim) - new hidden state\n",
    "            c_new: (1, hidden_dim) - new cell state\n",
    "        \"\"\"\n",
    "        # Calculate h_tilde\n",
    "        if not child_h:\n",
    "            h_sum = torch.zeros(1, self.hidden_dim).to(x.device)\n",
    "        else:\n",
    "            h_sum = torch.sum(torch.cat(child_h, dim=0), dim=0, keepdim=True)\n",
    "            \n",
    "        # Calculate i, o, u\n",
    "        iou = self.W_iou(x) + self.U_iou(h_sum)\n",
    "        i, o, u = torch.split(iou, self.hidden_dim, dim=1)\n",
    "        i, o, u = torch.sigmoid(i), torch.sigmoid(o), torch.tanh(u)\n",
    "        \n",
    "        # Caculate f, and new c\n",
    "        # c = i*u + sum(f_k * c_k)\n",
    "        c_new = i * u\n",
    "        \n",
    "        if child_h:\n",
    "            for h_k, c_k in zip(child_h, child_c):\n",
    "                f_k = torch.sigmoid(self.W_f(x) + self.U_f(h_k))\n",
    "                c_new += f_k * c_k\n",
    "                \n",
    "        h_new = o * torch.tanh(c_new)\n",
    "        return h_new, c_new\n",
    "\n",
    "class ReplyTreeRNN(nn.Module):\n",
    "    def __init__(self, plm_name, hidden_dim, num_classes, dropout=0.3):\n",
    "        super(ReplyTreeRNN, self).__init__()\n",
    "        # 1. PLM\n",
    "        self.plm = AutoModel.from_pretrained(plm_name)\n",
    "        self.plm_config = self.plm.config\n",
    "        embedding_dim = self.plm_config.hidden_size\n",
    "        \n",
    "        # 2. Tree-LSTM\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.treelstm = ChildSumTreeLSTMCell(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # 3. Classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, root_node, predictions_dict):\n",
    "        \"\"\"\n",
    "        root_node: TreeNode - the root of the tree to process\n",
    "        predictions_dict: dict - to store predictions for each node by mid\n",
    "        Returns:\n",
    "            None (predictions are stored in predictions_dict)\n",
    "        \"\"\"\n",
    "        def traverse(node):\n",
    "            # list to store child states\n",
    "            child_h = []\n",
    "            child_c = []\n",
    "            for child in node.children:\n",
    "                h, c = traverse(child)\n",
    "                child_h.append(h)\n",
    "                child_c.append(c)\n",
    "            \n",
    "            input_ids = node.input_ids.unsqueeze(0).to(DEVICE)\n",
    "            att_mask = node.attention_mask.unsqueeze(0).to(DEVICE)\n",
    "            \n",
    "            # PLM Forward\n",
    "            plm_out = self.plm(input_ids, attention_mask=att_mask)\n",
    "            # Pooler Output (CLS + Linear + Tanh)\n",
    "            # OR last_hidden_state[:, 0, :]\n",
    "            node_emb = plm_out.pooler_output # (1, 768)\n",
    "            node_emb = self.dropout(node_emb)\n",
    "            \n",
    "            # Tree-LSTM Forward\n",
    "            h, c = self.treelstm(node_emb, child_c, child_h)\n",
    "            \n",
    "            # Prediction\n",
    "            logits = self.classifier(h)\n",
    "            predictions_dict[node.mid] = logits\n",
    "            \n",
    "            return h, c\n",
    "\n",
    "        traverse(root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a12b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, trees, optimizer, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    random.shuffle(trees)\n",
    "    \n",
    "    step_loss = 0\n",
    "    node_count = 0\n",
    "    \n",
    "    for i, root in enumerate(tqdm(trees, desc=\"Training\", leave=False)):\n",
    "        preds = {}\n",
    "        model(root, preds) \n",
    "        \n",
    "        # Loss\n",
    "        loss = 0\n",
    "        valid_nodes = 0\n",
    "        \n",
    "        # Collect loss from all nodes\n",
    "        def collect_loss(node):\n",
    "            nonlocal loss, valid_nodes\n",
    "            if node.mid in preds:\n",
    "                label = torch.tensor([node.label], device=DEVICE)\n",
    "                loss += criterion(preds[node.mid], label)\n",
    "                valid_nodes += 1\n",
    "            for child in node.children:\n",
    "                collect_loss(child)\n",
    "        \n",
    "        collect_loss(root)\n",
    "        \n",
    "        if valid_nodes > 0:\n",
    "            loss = loss / valid_nodes # Loss average per node\n",
    "            loss = loss / accumulation_steps # Gradient accumulation\n",
    "            loss.backward()\n",
    "            step_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += step_loss\n",
    "            step_loss = 0\n",
    "            \n",
    "    # Final step if not divisible\n",
    "    if (i + 1) % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += step_loss\n",
    "        \n",
    "    return total_loss / len(trees)\n",
    "\n",
    "def evaluate(model, trees):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for root in trees:\n",
    "            preds = {}\n",
    "            model(root, preds)\n",
    "            \n",
    "            def collect(node):\n",
    "                if node.mid in preds:\n",
    "                    p = torch.argmax(preds[node.mid], dim=1).item()\n",
    "                    all_preds.append(p)\n",
    "                    all_labels.append(node.label)\n",
    "                for child in node.children:\n",
    "                    collect(child)\n",
    "            collect(root)\n",
    "            \n",
    "    if not all_labels: return 0.0, 0.0\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0003\n",
    "hidden_dim = 256\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ac97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_trees = train_trees + val_trees\n",
    "\n",
    "print(\"Training Final Model with Best Params...\")\n",
    "final_model = ReplyTreeRNN(\n",
    "    PLM_MODEL_PATH, \n",
    "    best_params['hidden_dim'], \n",
    "    NUM_CLASSES, \n",
    "    best_params['dropout']\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(final_model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train_one_epoch(final_model, full_train_trees, optimizer, accumulation_steps=8)\n",
    "    test_acc, test_f1 = evaluate(final_model, test_trees)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f}\")\n",
    "\n",
    "acc, f1 = evaluate(final_model, test_trees)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Macro-F1: {f1:.4f}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e6188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81459dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PowerBank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
