{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoModel, AutoTokenizer, logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    # Data params - \n",
    "    CSV_PATH = 'DATA_PATH' # modify as the actul data path      \n",
    "    TEXT_COL = 'text'          \n",
    "    LABEL_COL = 'sentiment'        \n",
    "    THREAD_COL = 'thread_id'   \n",
    "    TURN_COL = 'turn_index'    \n",
    "    PARENT_COL = 'parent_id'   \n",
    "    \n",
    "    # Model defaults\n",
    "    PLM_NAME = 'roberta-base'\n",
    "    MAX_LEN = 256             # Matched to baseline\n",
    "    MAX_TURNS = 50\n",
    "    \n",
    "    # Architecture Params\n",
    "    PROJ_DIM = 256\n",
    "    HIDDEN_DIM = 256          # Matched to baseline\n",
    "    DROPOUT = 0.3             # Matched to baseline\n",
    "    \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(Config.SEED)\n",
    "print(f\"Device set to: {Config.DEVICE}\")\n",
    "\n",
    "\n",
    "## 2. Data Loading\n",
    "def load_data_split(config):\n",
    "    if not os.path.exists(config.CSV_PATH):\n",
    "         raise FileNotFoundError(f\"CSV file not found at {config.CSV_PATH}\")\n",
    "\n",
    "    df = pd.read_csv(config.CSV_PATH)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    unique_labels = sorted(df[config.LABEL_COL].unique())\n",
    "    label_map = {l: i for i, l in enumerate(unique_labels)}\n",
    "    df['label_idx'] = df[config.LABEL_COL].map(label_map)\n",
    "    \n",
    "    threads = [group for _, group in df.groupby(config.THREAD_COL)]\n",
    "    train, test = train_test_split(threads, test_size=0.3, random_state=config.SEED)\n",
    "    val, test = train_test_split(test, test_size=0.5, random_state=config.SEED)\n",
    "    \n",
    "    all_labels = []\n",
    "    for t in train: all_labels.extend(t['label_idx'].values)\n",
    "    \n",
    "    if len(np.unique(all_labels)) < len(label_map):\n",
    "        weights = None\n",
    "    else:\n",
    "        cw = compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)\n",
    "        weights = torch.tensor(cw, dtype=torch.float).to(config.DEVICE)\n",
    "        print(f\"Class Weights: {weights}\")\n",
    "    \n",
    "    return train, val, test, len(label_map), weights\n",
    "\n",
    "class ThreadDataset(Dataset):\n",
    "    def __init__(self, thread_list, tokenizer, config):\n",
    "        self.threads = thread_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self): return len(self.threads)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df = self.threads[idx].sort_values(self.config.TURN_COL)\n",
    "        texts = df[self.config.TEXT_COL].astype(str).tolist()\n",
    "        enc = self.tokenizer(texts, padding='max_length', truncation=True, max_length=self.config.MAX_LEN, return_tensors='pt')\n",
    "        \n",
    "        turns = np.clip(df[self.config.TURN_COL].values, 0, self.config.MAX_TURNS-1) if self.config.TURN_COL in df.columns else np.arange(len(df))\n",
    "        parents = df[self.config.PARENT_COL].fillna(0).values if self.config.PARENT_COL in df.columns else np.zeros(len(df))\n",
    "        is_reply = (parents != 0).astype(int)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': enc['input_ids'],\n",
    "            'attention_mask': enc['attention_mask'],\n",
    "            'turn_ids': torch.tensor(turns, dtype=torch.long),\n",
    "            'reply_ids': torch.tensor(is_reply, dtype=torch.long),\n",
    "            'labels': torch.tensor(df['label_idx'].values, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': pad_sequence([b['input_ids'] for b in batch], batch_first=True, padding_value=0),\n",
    "        'attention_mask': pad_sequence([b['attention_mask'] for b in batch], batch_first=True, padding_value=0),\n",
    "        'turn_ids': pad_sequence([b['turn_ids'] for b in batch], batch_first=True, padding_value=0),\n",
    "        'reply_ids': pad_sequence([b['reply_ids'] for b in batch], batch_first=True, padding_value=0),\n",
    "        'labels': pad_sequence([b['labels'] for b in batch], batch_first=True, padding_value=-100)\n",
    "    }\n",
    "\n",
    "## 3. Model Architecture\n",
    "class LiquidCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W_tau, self.U_tau = nn.Linear(input_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_g, self.U_g = nn.Linear(input_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.epsilon = 1e-6 \n",
    "\n",
    "    def forward(self, z_i, h_prev):\n",
    "        # Tau calculation\n",
    "        tau = F.softplus(self.W_tau(z_i) + self.U_tau(h_prev)) + self.epsilon\n",
    "        # Update gate\n",
    "        g = torch.tanh(self.W_g(z_i) + self.U_g(h_prev))\n",
    "        # Liquid state update\n",
    "        h_new = (1.0 - (1.0 / tau)) * h_prev + g\n",
    "        return h_new\n",
    "\n",
    "class PLITSNet_Hybrid(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        # PLM backbone\n",
    "        self.plm = AutoModel.from_pretrained(config.PLM_NAME)\n",
    "        # Initially freeze\n",
    "        for p in self.plm.parameters(): p.requires_grad = False\n",
    "            \n",
    "        self.plm_dim = self.plm.config.hidden_size\n",
    "        self.proj = nn.Linear(self.plm_dim, config.PROJ_DIM)\n",
    "        self.dropout = nn.Dropout(config.DROPOUT)\n",
    "        \n",
    "        # Structure Embeddings\n",
    "        self.turn_emb = nn.Embedding(config.MAX_TURNS, 16)\n",
    "        self.reply_emb = nn.Embedding(2, 16)\n",
    "        \n",
    "        # Liquid Encoder\n",
    "        self.liquid = LiquidCell(config.PROJ_DIM + 32, config.HIDDEN_DIM)\n",
    "        self.hidden_dim = config.HIDDEN_DIM\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(config.HIDDEN_DIM + config.PROJ_DIM, num_classes)\n",
    "        self.next_step_pred = nn.Linear(config.HIDDEN_DIM, config.PROJ_DIM)\n",
    "\n",
    "    def unfreeze_plm(self):\n",
    "        print(\">>> Unfreezing PLM for Fine-tuning...\")\n",
    "        for p in self.plm.parameters(): p.requires_grad = True\n",
    "\n",
    "    def _get_embeddings(self, input_ids, attention_mask):\n",
    "        B, T, L = input_ids.shape\n",
    "        flat_out = self.plm(input_ids.view(-1, L), attention_mask=attention_mask.view(-1, L))\n",
    "        mask = attention_mask.view(-1, L).unsqueeze(-1).float()\n",
    "        # Mean pooling\n",
    "        c_i = (flat_out.last_hidden_state * mask).sum(1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "        e_i = self.proj(c_i) \n",
    "        return e_i.view(B, T, -1)\n",
    "\n",
    "    def forward_features(self, input_ids, attention_mask, turn_ids, reply_ids):\n",
    "        e_i = self._get_embeddings(input_ids, attention_mask)\n",
    "        e_i_drop = self.dropout(e_i)\n",
    "        \n",
    "        # Structure Fusion\n",
    "        s_i = torch.cat([self.turn_emb(turn_ids), self.reply_emb(reply_ids)], dim=-1)\n",
    "        z_i = torch.cat([e_i_drop, s_i], dim=-1) \n",
    "        \n",
    "        B, T, _ = z_i.shape\n",
    "        h = torch.zeros(B, self.hidden_dim).to(z_i.device)\n",
    "        h_seq = []\n",
    "        for t in range(T):\n",
    "            h = self.liquid(z_i[:, t, :], h)\n",
    "            h_seq.append(h)\n",
    "        h_seq = torch.stack(h_seq, dim=1)\n",
    "        \n",
    "        return h_seq, e_i_drop, e_i \n",
    "\n",
    "    def forward_pretrain(self, input_ids, attention_mask, turn_ids, reply_ids):\n",
    "        h_seq, _, e_i_raw = self.forward_features(input_ids, attention_mask, turn_ids, reply_ids)\n",
    "        preds = self.next_step_pred(h_seq) \n",
    "        preds_shifted = preds[:, :-1, :]\n",
    "        targets_shifted = e_i_raw[:, 1:, :] \n",
    "        return preds_shifted, targets_shifted\n",
    "\n",
    "    def forward_classify(self, input_ids, attention_mask, turn_ids, reply_ids):\n",
    "        h_seq, e_i_drop, _ = self.forward_features(input_ids, attention_mask, turn_ids, reply_ids)\n",
    "        # Concatenation for classification\n",
    "        logits = self.classifier(torch.cat([h_seq, e_i_drop], dim=-1))\n",
    "        return logits\n",
    "\n",
    "## 4. Training Logic\n",
    "def train_stage1_unsupervised(model, dataloader, epochs, device):\n",
    "    print(\"\\n--- Stage 1: Structure Learning (Frozen PLM) ---\")\n",
    "    # Unsupervised adaptation\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k!='labels'}\n",
    "            \n",
    "            preds, targets = model.forward_pretrain(inputs['input_ids'], inputs['attention_mask'],\n",
    "                                                    inputs['turn_ids'], inputs['reply_ids'])\n",
    "            \n",
    "            if preds.shape[1] > 0:\n",
    "                loss = criterion(preds, targets.detach()) \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{epochs} | MSE: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "def train_stage2_supervised(model, train_loader, val_loader, epochs, device, class_weights):\n",
    "    print(\"\\n--- Stage 2: Supervised Fine-tuning (Unfrozen PLM) ---\")\n",
    "    \n",
    "    # UNFREEZE PLM for better performance on small data\n",
    "    model.unfreeze_plm()\n",
    "    \n",
    "    plm_params = list(map(id, model.plm.parameters()))\n",
    "    head_params = filter(lambda p: id(p) not in plm_params, model.parameters())\n",
    "    \n",
    "    # Differential LR: Small for PLM, Large for Head\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': model.plm.parameters(), 'lr': 2e-5},\n",
    "        {'params': head_params, 'lr': 1e-3}\n",
    "    ])\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k!='labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model.forward_classify(inputs['input_ids'], inputs['attention_mask'],\n",
    "                                            inputs['turn_ids'], inputs['reply_ids'])\n",
    "            \n",
    "            loss = criterion(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Eval\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {k: v.to(device) for k, v in batch.items() if k!='labels'}\n",
    "                labels = batch['labels'].to(device)\n",
    "                logits = model.forward_classify(inputs['input_ids'], inputs['attention_mask'],\n",
    "                                                inputs['turn_ids'], inputs['reply_ids'])\n",
    "                mask = labels != -100\n",
    "                preds.extend(torch.argmax(logits, -1)[mask].cpu().numpy())\n",
    "                trues.extend(labels[mask].cpu().numpy())\n",
    "        \n",
    "        acc = accuracy_score(trues, preds)\n",
    "        f1 = f1_score(trues, preds, average='macro')\n",
    "        \n",
    "        # Log manually\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"  Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(train_loader):.4f} | Val F1: {f1:.4f} | LR: {current_lr:.2e}\")\n",
    "        \n",
    "        scheduler.step(f1)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_acc = acc\n",
    "\n",
    "    print(f\"--- Best Val F1: {best_f1:.4f} (Acc: {best_acc:.4f}) ---\")\n",
    "\n",
    "## 5. Execution\n",
    "train_threads, val_threads, test_threads, num_classes, weights = load_data_split(Config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.PLM_NAME)\n",
    "\n",
    "# Datasets\n",
    "train_ds = ThreadDataset(train_threads, tokenizer, Config)\n",
    "val_ds = ThreadDataset(val_threads, tokenizer, Config)\n",
    "test_ds = ThreadDataset(test_threads, tokenizer, Config)\n",
    "unsupervised_ds = ThreadDataset(train_threads + val_threads + test_threads, tokenizer, Config)\n",
    "\n",
    "# Dataloaders\n",
    "loader_args = {'batch_size': 8, 'collate_fn': collate_fn}\n",
    "unsupervised_loader = DataLoader(unsupervised_ds, shuffle=True, **loader_args)\n",
    "train_loader = DataLoader(train_ds, shuffle=True, **loader_args)\n",
    "val_loader = DataLoader(val_ds, shuffle=False, **loader_args)\n",
    "test_loader = DataLoader(test_ds, shuffle=False, **loader_args)\n",
    "\n",
    "# Model\n",
    "model = PLITSNet_Hybrid(Config, num_classes).to(Config.DEVICE)\n",
    "\n",
    "# 1. Warm-up Structure Learning (Frozen PLM)\n",
    "train_stage1_unsupervised(model, unsupervised_loader, epochs=10, device=Config.DEVICE)\n",
    "\n",
    "# 2. Full Fine-Tuning (Unfrozen PLM, Differential LR)\n",
    "train_stage2_supervised(model, train_loader, val_loader, epochs=15, device=Config.DEVICE, class_weights=weights)\n",
    "\n",
    "# 3. Final Test\n",
    "model.eval()\n",
    "preds, trues = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {k: v.to(Config.DEVICE) for k, v in batch.items() if k!='labels'}\n",
    "        labels = batch['labels'].to(Config.DEVICE)\n",
    "        logits = model.forward_classify(inputs['input_ids'], inputs['attention_mask'],\n",
    "                                        inputs['turn_ids'], inputs['reply_ids'])\n",
    "        mask = labels != -100\n",
    "        preds.extend(torch.argmax(logits, -1)[mask].cpu().numpy())\n",
    "        trues.extend(labels[mask].cpu().numpy())\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"Final Test Accuracy: {accuracy_score(trues, preds):.4f}\")\n",
    "print(f\"Final Test Macro-F1: {f1_score(trues, preds, average='macro'):.4f}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8aaf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3c0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654b99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c8089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e3df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39f68b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
